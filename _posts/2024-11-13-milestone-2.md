---
layout: post
title: Milestone 2
---

## 2. Feature Engineering I

### 2.1 Histogram of shot counts binned by shot distance and shot angle

NOTE: The 'Non-Goals' are shots classified as 'shot-on-goal' and 'missed-shots' in our data for seasons from 2016-2017 upto 2019-2020(4 seasons). Additionally, shot angles are higher for shots that are taken head-on. For example, a shot angle of 90 degrees means it is directed along the same line as taking a shot from the centre of the pitch towards goal.

Plotting histogram of shot counts binned by shot distance:

<img src="/assets/images/Feature1_binshots_dist.png" alt="Goals by distance">
<img src="/assets/images/Feature1_binshotNg_distance.png" alt="Non-goals by distance">

  - In the 'Goals by distance' distribution, most goal counts cluster around smaller values of distances (0 to 25 feet), with an exponential decrease in counts with increase in distance.

  - However, with the 'Non-Goals by distance' distribution, the shot counts are more spread out over the range of distances except for very large distances (distance> 60 feet and distance< 10 feet). The shot counts in the 10 to 60 feet range has some resemblances to a uniform distribution.

Plotting histogram of shot counts binned by shot angle:

<img src="/assets/images/Feature1_binshotG_angle.png" alt="Goals by angle">
<img src="/assets/images/Feature1_binshotsNg_angle.png" alt="Non-goals by angle">

  - In the first plot of shots that led to goals versus the angle of the shot, there is an exponential decrease in the in goal count with decrease in shot angle. Most counts are clustered around the 80 to 90 degree shot angle range.

  - However in the second plot, there is a smoother decrease in shot counts (like an ascending bell-curve) from the 80 degree mark.

  - A common characteristic between the two plots is that there is a large cluster of counts for shot angles >80 and very low counts for shot angles lesser than 40 degrees.


2D histogram of shot distance vs shot angle (goals and non-goals included):
<img src="/assets/images/Feature1_2Dhist.png" alt="2D histogram">

  - The darkest (densest) regions are at short distances (0–20 feet) and high shot angles (80–90 degrees).
  - This suggests that most shot counts are those taken from close range and directly in front of the net.
  - Additionally, the plot indicates also that with increase in distance to the goal, shots are taken at a tighter angles (less head-on towards goal).

### 2.2 Plotting Goal Rate (Goal/(No Goals + Goals)) binned by distance and shot angle

Goal rate by shot distance:

<img src="/assets/images/Feature1_bingoals_distance.png" alt="Goal rate by distance">

### Observations
- The highest goal rates occur in the smallest distance bin ( 0–10 feet), with a goal rates exceeding 0.16.
Generally the goal rates are higher for smaller distances from goal.

- There is a exponential decline in goal rate as the distance increases (0 - 60 feet). Beyond 40 feet, the goal rate becomes very low (below 0.05), indicating that long-range shots have much lower chances of success.

- From the 80 feet range (80 - 100 feet), there seems to be a slight increase in goal rate over distance. This is potentially due to the presence of wrong data(wrong x, y coordinates) in contrast to the actual events of the game, which is explained further in section 2.3.

Goal rate by shot angle:

<img src="/assets/images/Feature1_bingoals_angle.png" alt="Goal rate by angle">

### Observations

- The highest goal rate is observed in the largest shot angle bin (80–90 degrees), with a goal rate above 0.10. These are most likely head-on shots directly in front of the goal.
- Another slight rise in goal rate is seen at small angles (0–10 degrees), possibly reflecting odd deflections or rebounds.

- The goal rate is significantly lower at intermediate shot angles (e.g., 20–70 degrees), with values clustering around 0.02–0.05. These shots are among the least likely to be goals possibly due to ease at which saves can be made.

### 2.3 Plotting Histograms of empty and non-empty goals

<img src="/assets/images/Feature1_emptygoals_dist.png" alt="Empty goals by distance">

<img src="/assets/images/Feature1_nonempty_distance.png" alt="Non Empty goals by distance">

### Observations
- It is easy to observe that the counts of goals scored as 'Empty net' goals are much lower (max count is 120 in a bin) than the counts of goals in 'Non-empty'(max count of 7000 in a bin) goal situations, across the 4 seasons. This implies that Empty net situations are not a common occurence

- There is a resemblance in the shape of the distribution between the two plots for distances between 0 to 20 feet. However after this mark, the numbers of goals scored on 'Non-Empty' goals sees a steep decline in count, which makes sense as it is harder to score a long range goal towards a guarded net. 

- This is in contrast to the distribution of goal counts beyond 20 feet for 'Empty Net' situations- although there is a decrease in goal counts with increase in distance, it is much more gradual, indicating that scoring a goal at an 'Empty Net' does not get too much harder with an increase in distance.

### Investigating long distance goals on 'Non-Empty' goals

- The plot for 'Non-empty' goals shows a very small count for goals scored beyond 80 feet, however - still not 0. Upon further investigation, a few gameIds were investigated and we found that some goal entries had mislabelled X and Y coordinates for goals both in the raw JSON data extracted using the NHL play-by-play API and even on the play-by-play plot for the event on the  [NHL GameCenter website](https://www.nhl.com/gamecenter/mtl-vs-phi/2024/10/27/2024020138) for the same game.

We present two examples as proof of this scenario:

- GAME ID ```2016020349```

The JSON entry and the Offical NHL play-by-play plot for the long distance (shot taken near defensive zone) 'Non-Empty' goal is shown below:

<img src="/assets/images/Feature1_JSONgame1.png" alt="JSON for 2016020349">

<img src="/assets/images/Feature1_Game1play.png" alt="NHL plot for 2016020349">

However the actual goal was scored from a different coordinate:

<img src="/assets/images/Feature1_Game1Actual.png" alt="Actual shot image 2016020349">

The replay of the game and the actual event can be seen at the 6:05 mark in this [video](https://www.youtube.com/watch?v=Gs2pgJQhD24)

- GAME ID ```2017020870```

Official NHL play-by-play plot for the mislabelled goal:
<img src="/assets/images/Feature1_Game2play.png" alt="NHL plot for 2017020870">

Actual goal was scored from near range:
<img src="/assets/images/Feature1_Game2Actual.png" alt="Actual image for 2017020870">

The actual moment of the game can be seen at the 3:05 mark in this [video](https://www.youtube.com/watch?v=KQp8-xHJa98&t=199s)

## 3. Baseline Models

1\ Using your dataset (remember, don’t touch the test set!), create a training and validation split however you feel is appropriate. Using only the distance feature, train a Logistic Regression classifier with the completely default settings, i.e.:

```python
clf = LogisticRegression()
clf.fit(X, y)
```

Evaluate the accuracy (i.e. correctly predicted / total) of your model on the validation set. What do you notice? Look at the predictions and discuss your findings. What could be a potential issue? Include these discussions in your blog post.

<img src="/assets/images/Baseline Logistic Regression_Loss.png" alt="Baseline Loss">

<img src="/assets/images/Baseline Logistic Regression_Accuracy.png" alt="Baseline Accuracy">

In this simple logistic regression model, we do 20 times of cross validations. From the above 2 plots in loss and accuracy, we can see that the accuracy and loss look "*seemingly good*". The accuracy is around 0.93 and the loss is around 0.24. However, the prediction proportion of goal and non goal is abnormal.

| Steps | Count for Goal prediction| Count fo Non goal prediction |
|------------|------|---------|
| Step 1     | 0 |   85510  |
| Step 2     | 0 |   85510  |
| Step 3     | 0 |   85510  |
| ...     | ... |   ...  |
| Step 20     | 0 |   85510  |

All the predictions are non goals while the accuracy is still high. The model is what we expect to generalize the outcome of goal and non goal as the model is not doing any prediction on the goal.

The potential issue is that the performance metric is not telling an valid evaluation on the model, hidding a high variance due to
- Feature limitation: the model is too simple to capture the complexity of the data. Using only the distance cannot capture all necessary information to predict if a goal will be scored.
- Class imbalance: the number of goals and shots are biased, which makes the model biased towards the majority class (not a goal).
<img src="/assets/images/Distance Histogram.png" alt="Distance Histogram">
- Threshold limitation: the model is not able to predict the goal as the threshold is not set specifically. The threshold is set to 0.5 by default, which is not suitable for this scenario.
- Model limitation: the logistic regression model is not suitable for this task as it is a linear model and the relationship between the features and the target is possibly non-linear.

2\ Based on your findings in Q1, we should explore other ways of evaluating our model. The first thing to note is that we are not actually interested in the binary prediction of whether a shot is a goal or not - we are interested in the probability that the model assigns to it (recall that we’re interested in the notion of expected goals). Scikit-learn provides this to you via the predict_proba(...) method; make sure you take the probabilities of the class that you care about! You will produce four figures (one curve per model per plot) to probe our model’s performance. Make sure you are using the probabilities obtained on the validation set:
   - Receiver Operating Characteristic (ROC) curves and the AUC metric of the ROC curve. Include a random classifier baseline, i.e. each shot has a 50% chance of being a goal.
   - The goal rate (#goals / (#no_goals + #goals)) as a function of the shot probability model percentile, i.e. if a value is the 70th percentile, it is above 70% of the data.
   - The cumulative proportion of goals (not shots) as a function of the shot probability model percentile.
   - The reliability diagram (calibration curve). Scikit-learn provides functionality to create a reliability diagram in a few lines of code; check out the CalibrationDisplay API (specifically the .from_estimator() or .from_predictions() methods) for more information.

An example of what is expected for (b) and (c) are shown in Fig. 1. Do not include all of these in your blog post yet, as you will add a few more curves in the next section.

3\ Now train two more Logistic Regression classifiers using the same setup as above, but this time on the angle feature, and then both distance and angle. Produce the same three curves as described in the previous section for each model. Including the random baseline, you should have a total of 4 lines on each figure:
   - Logistic Regression, trained on distance only (already done above)
   - Logistic Regression, trained on angle only
   - Logistic Regression, trained on both distance and angle
   - Random baseline: rather than training a classifier, the predicted probability is sampled from a uniform distribution, i.e. yiU(0,1)
   Include these four figures (each with four curves) in your blogpost. In a few sentences, discuss your interpretation of these results.

<img src="/assets/images/Baseline_ROC AUC.png" alt="Baseline ROC AUC">

<img src="/assets/images/Baseline_Percentile.png" alt="Baseline Percentile">

<img src="/assets/images/Baseline_Cumu.png" alt="Baseline Cumulative portion">

<img src="/assets/images/Baseline_Calibration.png" alt="Baseline Calibration">

Links (TO UPDATE):
- [Experiment 0_Logistic Regression (distanct)](https://wandb.ai/IFT6758-2024-B04/yh-baseline-testing/runs/uw2jig0f)
- [Experiment 1_Logistic Regression (shotAngle)](https://wandb.ai/IFT6758-2024-B04/yh-baseline-testing/runs/ok2zyfjo)
- [Experiment 2_Logistic Regression (distanct, shotAngle)](https://wandb.ai/IFT6758-2024-B04/yh-baseline-testing/runs/vmyuebrm)

From these 4 plots, we can see
- The ROC AUC curve
  - The model using only distance has the worst performance (AUC = 0.62), while the models with angle only and with both distance and angle perform similarly (AUC = 0.69).
  - At a false positive rate (FPR) above 0.8, the model using only distance performs even worse than the random classifier, suggesting poor reliability in high FPR scenarios.
- The goal rate vs probability percentile plot
  - The random classifier exhibits a flat and low goal rate across percentiles, reflecting its lack of predictive power.
  - The model using angle only has a peculiar increase in goal rate for shot probabilities below the 20th percentile, which could indicate overfitting or irregular calibration in this range.
  - All the baseline models, generally show a general trend of increasing goal rate as the predicted probability percentile increases.
- The cumulative of goals vs probability percentile plot
  - The cumulative goal plot aligns with the trends seen in the ROC curve, showing that models using angle (alone or combined with distance) outperform the model with distance only.
  - The random classifier trails significantly behind the other models, as expected.
- The reliability curve
  - All baseline models show limited calibration with predicted probabilities confined to a narrow range, deviating from the "perfectly calibrated" diagonal.
  - The random classifier's calibration is notably poor, with predictions far from the perfect calibration line, further affirming its unsuitability for this task.

The model using only distance performs the worst, while the models using angle (alone or combined with distance) demonstrate comparable and better performance. This indicates that angle is a more significant feature than distance in predicting goals.

The random classifier is ineffective as it lacks any meaningful predictive tendencies or calibration.

The rise in goal rate at low percentiles for the distance-only model indicates potential issues with calibration or feature representation that merit further investigation.

4\ Next to the figures, include links to the three experiment entries in your Wandb projects that produced these three models. Save the three models to the three experiments on Wandb (example here) and register them with some informative tags, as you will need it for the final section.

## 4. Feature Engineering II

### 4.1
. Note that you can give these features whatever informative name that you would like.

From the already tidied data, include the following features:
- Game seconds
- Game period
- Coordinates (x,y, separate columns)
- Shot distance
- Shot angle
- Shot type

### 4.2
 Now, to each shot, add information from the previous events. Note that this could be any event, not just shots. This means you will likely have to update your code which produced the tidied data to now also look at other event types.

To each shot, add the following information from the immediately preceding event as four new features:
- Last event type
- Coordinates of the last event (x, y, separate columns)
- Time from the last event (seconds)
- Distance from the last event

### 4.3
 With this new information, we will create a few more features which try to quantify a few more interesting things about the state of the play. Create the following three features:
   - Rebound (bool): True if the last event was also a shot, otherwise False
   - Change in shot angle; only include if the shot is a rebound, otherwise 0.
   - “Speed”: defined as the distance from the previous event, divided by the time since the previous event.

### 4.4
 (Bonus 5%) As a bonus, compute a few features regarding the power-play situation. Include:
   - Time since the power-play started (seconds), reset this to 0 when the power-play expires.
   Note that there are 2 minute minor, 4 minute double minor, and 5 minute major penalties. Read the documentation to understand how each penalty expires.
   Power-plays can stack, i.e. if the penalized team takes another penalty, they will be down 2 players until the penalty expires, then down 1 until the second expires. In this case, don’t reset the timer; simply keep counting until all penalties expire.
   - Number of friendly non-goalie skaters on the ice
   - Number of opposing non-goalie skaters on the ice

This will be a bit tricky, as you will have to keep track of each penalty and figure out when the penalty expires to reset the number of players on the ice as well as the time since the power-play started. You do have access to a list of all penalty events by index in the raw data:


### 4.5. Dataset logging into W&B
[Team B4 Dataset experiement in W&B](https://wandb.ai/IFT6758-2024-B04/ms2-q4-dataset-V0/artifacts/dataset/wpg_v_wsh_2017021065/v1/files/wpg_v_wsh_2017021065.table.json)

<table>
  <thead>
    <tr>
      <th>Feature</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Distance</td>
      <td>Distance between the current shot and the target goal</td>
    </tr>
    <tr>
      <td>shotAngle</td>
      <td>Angle of the current shot to the goal (90 deg is straight line, 0 deg is beside the goal)</td>
    </tr>
    <tr>
      <td>isGoal</td>
      <td>1: Indicates a goal was scored with the current shot</td>
    </tr>
    <tr>
      <td>isEmpty</td>
      <td>1: indicates that there was no goaler in the target goal when the current shot was taken</td>
    </tr>
    <tr>
      <td>lastEventType</td>
      <td>Type of the event that occured before the current shot</td>
    </tr>
    <tr>
      <td>timeSinceLastEvent</td>
      <td>Time (in seconds) since the last event</td>
    </tr>
    <tr>
      <td>distFromLastEvent</td>
      <td>Distance (in feet) between the current and the previous events</td>
    </tr>
    <tr>
      <td>rebound</td>
      <td>1: Indicates the current shot is a rebound from the previous shot</td>
    </tr>
    <tr>
      <td>changeInShotAngle</td>
      <td>Difference of angle between the current shot and the previous shot</td>
    </tr>
    <tr>
      <td>speed</td>
      <td>Estimated speed of the puck between current and previous shots (assuming straight line)</td>
    </tr>
  </tbody>
</table>
'shotAngle', 'isGoal', 'isEmpty', 'lastEventType', 'timeSinceLastEvent', 'distFromLastEvent', 'rebound', 'changeInShotAngle', 'speed'

Wandb provides some functionality to track datasets, either as raw artifacts that you manually save, or automatically uploading DataFrame/tables for you. It also has some nice functionality to compute dataset hashes, allowing you to verify if you’re training on the same data as others. You can find more info about this functionality in their documentation. You are not required to use this feature for the majority of your project (you can if you’d like!). However, for the purposes of this section, we will be asking you to submit a small subset (one specific game) of your final dataframe with all of your final features. Filter your dataset to the Winnipeg vs Washington game on March 12, 2018. This game has the game ID “2017021065”.  Upload the filtered DataFrame with all of the features that you created as a CSV using the Table(...) class; keep the name as 'wpg_v_wsh_2017021065'. Here’s example code for uploading dataset:

In your blogpost, add a list of all of the features that you created for this section. List each feature by both the column name in your dataframe AND a simple human-readable explanation (i.e. game_sec: Total number of seconds elapsed in the game). If you created any novel features, briefly describe what they are. Add a link to the experiment which stores the filtered DataFrame artifact for the specified game. Note that there should only be one DataFrame logged with this name. See the appendix for an example of what we are looking for.

## 5. Advanced Models
For each of the following questions, the same four figures as in part 3 will be used as quantitative metrics:
- ROC/AUC
- Goal rate vs probability percentile
- Cumulative proportion of goals vs probability percentile
- Reliability curve
Where a question asks you to record model metrics, add a corresponding curve to each of these four figures. Note that these are four new figures, i.e. you do not need to overlay the curves on the same figures you produced for part 3.

1. Train an XGBoost classifier using the same dataset using only the distance and angle features (similar to part 3). Don’t worry about hyperparameter tuning yet, this will just serve as a comparison to the baseline before we add more features. Add the corresponding curves to the four figures in your blog post. Briefly (few sentences) discuss your training/validation setup, and compare the results to the Logistic Regression baseline. Include a link to the relevant Wandb entry for this experiment, but you do not need to log this model to the model registry.

2. Now, train an XGBoost classifier using all of the features you created in Part 4 and do some hyperparameter tuning to try to find the best performing model with all of these features. In your blog post, discuss your hyperparameter tuning setup, and include figures to substantiate your choice of hyperparameters. For example, you could select appropriate metrics and do a grid search with cross validation. Once tuned, include curves corresponding to the best model to the four figures in your blog post, and briefly compare the results to the XGBoost baseline. Include a link to the relevant Wandb entry for this experiment, and log this model to the model registry.

3. Finally, explore using some feature selection techniques to see if you can simplify your input features. A number of features carry correlated information, so you can try to see if some of them are redundant. You can try some of the feature selection techniques discussed in class; many of these are implemented for you by scikit-learn.. You could also use a library like SHAP to try to interpret what feature your model relies on the most. Discuss the feature selection strategies that you tried, and what was the most optimal set of features that you came up with. Include some figures to substantiate your claims. Once you’ve found the optimal set of features via hyperparameter tuning/cross validation, if the feature set is different than what was used for Q2 of this section, include curves corresponding to the best model to the four figures in your blog post, and briefly compare the results to the XGBoost baseline. Include a link to the relevant Wandb entry for this experiment, and log this model to the model registry.


## 6. Give it your best shot!

1. In your blog post, discuss the various techniques and methods you tried. Include the same four figures as in Part 3 (ROC/AUC curve, goal rate vs probability percentile, cumulative proportion of goals vs probability percentile, and the reliability curve). Quantitative metrics are only required for a few set of experiments, so you only need to include a few curves on each plot (eg. things that you found interesting, or models that performed particularly well). Make sure to include and highlight what you consider your best ‘final’ model. For methods that weren’t too successful or interesting, you can just include them as short qualitative discussion.

2. Next to the figures, include links to the experiment entry in your Wandb projects that you included quantitative metrics for (around 3-4). Log the models to the experiments on Wandb (example here) and register them with some informative tags.


## 7. Evaluate on test set
For each the following questions, you will need to evaluate a total of five models:
- The three logistic regression models you saved in part 3
- The best XGBoost model you saved in part 5
- Your best overall model that you saved in part 6
and produce the same four figures as in part 3 (single figure for each, with 5 curves):
- ROC/AUC
- Goal rate vs probability percentile
- Cumulative proportion of goals vs probability percentile
- Reliability curve

Note that you’re more than welcome to evaluate more than the five specified above if you’d like! We only require that you evaluate the 5 specified models in order to get full marks. For the two questions below we expect 3-5 sentences each, but you’re welcome to go over this if you have interesting things to discuss.

1. Test your 5 models on the untouched 2020/21 regular season dataset. In your blogpost, include the four figures described above. Discuss your results and observations on the test set. Do your models perform as well on the test set as you did on your validation set when building your models? Do any models perform better or worse than you expected?

2. Test your 5 models on the untouched 2020/21 playoff games. In your blogpost, include the four figures described above. Discuss your results and observations on this test set. Are there any differences to the regular season test set or do you get similar ‘generalization’ performance?
   Group Evaluations



