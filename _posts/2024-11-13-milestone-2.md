---
layout: post
title: Milestone 2
---

## 2. Feature Engineering I
1. Using your training dataset create a tidied dataset for each SHOT/GOAL event, with the following columns (you can name them however you want):
   - Distance from net
   - Angle from net
   - Is goal (0 or 1)
   - Empty Net (0 or 1; you can assume NaNs are 0)
   Note that his would be found in the event’s description as “typecode”: 505 I think

You can approximate the net as a single point (i.e. you don’t need to account for the width of the net when computing the distance or angle). You should be able to create this easily using the functionality you implemented for tidying data in Milestone 1, as you will only need the (x, y) coordinates for each shot/goal event. Create and include the following figures in your blogpost and briefly discuss your observations (few sentences):
A histogram of shot counts (goals and no-goals separated), binned by distance
A histogram of shot counts (goals and no-goals separated), binned by angle
A 2D histogram where one axis is the distance and the other is the angle. You do not need to separate goals and no-goals.
Hint: check out jointplots.
As always, make sure all of your axes are labeled correctly, and you make the appropriate choice of axis scale.

2. Now, create two more figures relating the goal rate, i.e. #goals / (#no_goals + #goals), to the distance, and goal rate to the angle of the shot. Include these figures in your blogpost and briefly discuss your observations.

3. Finally, let’s do some quick checks to see if our data makes sense. Unfortunately we don’t have time to do automated anomaly detection, but we can use our “domain knowledge” for some quick sanity checks! The domain knowledge is that “it is incredibly rare to score a non-empty net goal on the opposing team from within your defensive zone”. Knowing this, create another histogram, this time of goals only, binned by distance, and separate empty net and non-empty net events. Include this figure in your blogpost and discuss your observations. Can you find any events that have incorrect features (e.g. wrong x/y coordinates, wrong shot type, etc.)? If yes, prove that one event has incorrect features. If you don’t find any that seem anomalous, briefly describe a few candidate events you inspected and show why they make sense.
Hint: the NHL gamecenter usually has video clips of goals for every game.
Don’t worry too much about this if you don’t find anything, just make a convincing case!

## 3. Baseline Models

1. Using your dataset (remember, don’t touch the test set!), create a training and validation split however you feel is appropriate. Using only the distance feature, train a Logistic Regression classifier with the completely default settings, i.e.:

clf = LogisticRegression()
clf.fit(X, y)

Evaluate the accuracy (i.e. correctly predicted / total) of your model on the validation set. What do you notice? Look at the predictions and discuss your findings. What could be a potential issue? Include these discussions in your blog post.

2. Based on your findings in Q1, we should explore other ways of evaluating our model. The first thing to note is that we are not actually interested in the binary prediction of whether a shot is a goal or not - we are interested in the probability that the model assigns to it (recall that we’re interested in the notion of expected goals). Scikit-learn provides this to you via the predict_proba(...) method; make sure you take the probabilities of the class that you care about! You will produce four figures (one curve per model per plot) to probe our model’s performance. Make sure you are using the probabilities obtained on the validation set:
   - Receiver Operating Characteristic (ROC) curves and the AUC metric of the ROC curve. Include a random classifier baseline, i.e. each shot has a 50% chance of being a goal.
   - The goal rate (#goals / (#no_goals + #goals)) as a function of the shot probability model percentile, i.e. if a value is the 70th percentile, it is above 70% of the data.
   - The cumulative proportion of goals (not shots) as a function of the shot probability model percentile.
   - The reliability diagram (calibration curve). Scikit-learn provides functionality to create a reliability diagram in a few lines of code; check out the CalibrationDisplay API (specifically the .from_estimator() or .from_predictions() methods) for more information.

An example of what is expected for (b) and (c) are shown in Fig. 1. Do not include all of these in your blog post yet, as you will add a few more curves in the next section.

3. Now train two more Logistic Regression classifiers using the same setup as above, but this time on the angle feature, and then both distance and angle. Produce the same three curves as described in the previous section for each model. Including the random baseline, you should have a total of 4 lines on each figure:
   - Logistic Regression, trained on distance only (already done above)
   - Logistic Regression, trained on angle only
   - Logistic Regression, trained on both distance and angle
   - Random baseline: rather than training a classifier, the predicted probability is sampled from a uniform distribution, i.e. yiU(0,1)
   Include these four figures (each with four curves) in your blogpost. In a few sentences, discuss your interpretation of these results.

4. Next to the figures, include links to the three experiment entries in your Wandb projects that produced these three models. Save the three models to the three experiments on Wandb (example here) and register them with some informative tags, as you will need it for the final section.


## 4. Feature Engineering II

1. Note that you can give these features whatever informative name that you would like.

From the already tidied data, include the following features:
- Game seconds
- Game period
- Coordinates (x,y, separate columns)
- Shot distance
- Shot angle
- Shot type

2. Now, to each shot, add information from the previous events. Note that this could be any event, not just shots. This means you will likely have to update your code which produced the tidied data to now also look at other event types.

To each shot, add the following information from the immediately preceding event as four new features:
- Last event type
- Coordinates of the last event (x, y, separate columns)
- Time from the last event (seconds)
- Distance from the last event

3. With this new information, we will create a few more features which try to quantify a few more interesting things about the state of the play. Create the following three features:
   - Rebound (bool): True if the last event was also a shot, otherwise False
   - Change in shot angle; only include if the shot is a rebound, otherwise 0.
   - “Speed”: defined as the distance from the previous event, divided by the time since the previous event.

4. (Bonus 5%) As a bonus, compute a few features regarding the power-play situation. Include:
   - Time since the power-play started (seconds), reset this to 0 when the power-play expires.
   Note that there are 2 minute minor, 4 minute double minor, and 5 minute major penalties. Read the documentation to understand how each penalty expires.
   Power-plays can stack, i.e. if the penalized team takes another penalty, they will be down 2 players until the penalty expires, then down 1 until the second expires. In this case, don’t reset the timer; simply keep counting until all penalties expire.
   - Number of friendly non-goalie skaters on the ice
   - Number of opposing non-goalie skaters on the ice

This will be a bit tricky, as you will have to keep track of each penalty and figure out when the penalty expires to reset the number of players on the ice as well as the time since the power-play started. You do have access to a list of all penalty events by index in the raw data:


5. Wandb provides some functionality to track datasets, either as raw artifacts that you manually save, or automatically uploading DataFrame/tables for you. It also has some nice functionality to compute dataset hashes, allowing you to verify if you’re training on the same data as others. You can find more info about this functionality in their documentation. You are not required to use this feature for the majority of your project (you can if you’d like!). However, for the purposes of this section, we will be asking you to submit a small subset (one specific game) of your final dataframe with all of your final features. Filter your dataset to the Winnipeg vs Washington game on March 12, 2018. This game has the game ID “2017021065”.  Upload the filtered DataFrame with all of the features that you created as a CSV using the Table(...) class; keep the name as 'wpg_v_wsh_2017021065'. Here’s example code for uploading dataset:

In your blogpost, add a list of all of the features that you created for this section. List each feature by both the column name in your dataframe AND a simple human-readable explanation (i.e. game_sec: Total number of seconds elapsed in the game). If you created any novel features, briefly describe what they are. Add a link to the experiment which stores the filtered DataFrame artifact for the specified game. Note that there should only be one DataFrame logged with this name. See the appendix for an example of what we are looking for.

## 5. Advanced Models
For each of the following questions, the same four figures as in part 3 will be used as quantitative metrics:
- ROC/AUC
- Goal rate vs probability percentile
- Cumulative proportion of goals vs probability percentile
- Reliability curve
Where a question asks you to record model metrics, add a corresponding curve to each of these four figures. Note that these are four new figures, i.e. you do not need to overlay the curves on the same figures you produced for part 3.

1. Train an XGBoost classifier using the same dataset using only the distance and angle features (similar to part 3). Don’t worry about hyperparameter tuning yet, this will just serve as a comparison to the baseline before we add more features. Add the corresponding curves to the four figures in your blog post. Briefly (few sentences) discuss your training/validation setup, and compare the results to the Logistic Regression baseline. Include a link to the relevant Wandb entry for this experiment, but you do not need to log this model to the model registry.

2. Now, train an XGBoost classifier using all of the features you created in Part 4 and do some hyperparameter tuning to try to find the best performing model with all of these features. In your blog post, discuss your hyperparameter tuning setup, and include figures to substantiate your choice of hyperparameters. For example, you could select appropriate metrics and do a grid search with cross validation. Once tuned, include curves corresponding to the best model to the four figures in your blog post, and briefly compare the results to the XGBoost baseline. Include a link to the relevant Wandb entry for this experiment, and log this model to the model registry.

3. Finally, explore using some feature selection techniques to see if you can simplify your input features. A number of features carry correlated information, so you can try to see if some of them are redundant. You can try some of the feature selection techniques discussed in class; many of these are implemented for you by scikit-learn.. You could also use a library like SHAP to try to interpret what feature your model relies on the most. Discuss the feature selection strategies that you tried, and what was the most optimal set of features that you came up with. Include some figures to substantiate your claims. Once you’ve found the optimal set of features via hyperparameter tuning/cross validation, if the feature set is different than what was used for Q2 of this section, include curves corresponding to the best model to the four figures in your blog post, and briefly compare the results to the XGBoost baseline. Include a link to the relevant Wandb entry for this experiment, and log this model to the model registry.


## 6. Give it your best shot!

1. In your blog post, discuss the various techniques and methods you tried. Include the same four figures as in Part 3 (ROC/AUC curve, goal rate vs probability percentile, cumulative proportion of goals vs probability percentile, and the reliability curve). Quantitative metrics are only required for a few set of experiments, so you only need to include a few curves on each plot (eg. things that you found interesting, or models that performed particularly well). Make sure to include and highlight what you consider your best ‘final’ model. For methods that weren’t too successful or interesting, you can just include them as short qualitative discussion.

2. Next to the figures, include links to the experiment entry in your Wandb projects that you included quantitative metrics for (around 3-4). Log the models to the experiments on Wandb (example here) and register them with some informative tags.


## 7. Evaluate on test set
For each the following questions, you will need to evaluate a total of five models:
- The three logistic regression models you saved in part 3
- The best XGBoost model you saved in part 5
- Your best overall model that you saved in part 6
and produce the same four figures as in part 3 (single figure for each, with 5 curves):
- ROC/AUC
- Goal rate vs probability percentile
- Cumulative proportion of goals vs probability percentile
- Reliability curve

Note that you’re more than welcome to evaluate more than the five specified above if you’d like! We only require that you evaluate the 5 specified models in order to get full marks. For the two questions below we expect 3-5 sentences each, but you’re welcome to go over this if you have interesting things to discuss.

1. Test your 5 models on the untouched 2020/21 regular season dataset. In your blogpost, include the four figures described above. Discuss your results and observations on the test set. Do your models perform as well on the test set as you did on your validation set when building your models? Do any models perform better or worse than you expected?

2. Test your 5 models on the untouched 2020/21 playoff games. In your blogpost, include the four figures described above. Discuss your results and observations on this test set. Are there any differences to the regular season test set or do you get similar ‘generalization’ performance?
   Group Evaluations



